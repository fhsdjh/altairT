{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Анализ Текущих Фич и Сравнение с Конкурентами"
      ],
      "metadata": {
        "id": "Q1Lre7gyDw1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Текущие фичи Джуниор:**\n",
        "\n",
        "Миссия Джуниор-ассистента — помочь сделать погружение ребенка в мир финансов мягче и интереснее. Например, если ребенок начинает интересоваться деньгами, то ИИ-помощник объяснит, как научиться копить, как работают банки, что такое инфляция.\n",
        "\n",
        "Джуниор ассистент умеет: помочь узнать что-то новое, помочь разобраться в сложной теме по любому школьному предмету, с помощью наводящих вопросов помочь ребенку с домашними заданиями, выслушать и дать полезный совет, развеселить и предложить сыграть в игру, поддержать беседу с ребенком на любую тему, при большом количестве негативных запросов от ребенка ассистент обучен рекомендовать ему обратиться к специалистам по телефону доверия.\n",
        "\n",
        "Эти фичи помогают вести диалог в режиме реального времени и понимать детей, подстраиваясь под запросы и настроение собеседника.\n",
        "\n",
        "**Сравнение с конкурентами:**\n",
        "\n",
        "Алиса от Яндекса, Маруся от VK, Siri от Apple: Эти ассистенты обладают широким спектром возможностей, но часто не адаптированы специально для детей. Не хватает контроля контента, образовательной направленности и простоты взаимодействия.\n",
        "\n",
        "По результатам тестов, при работе на моделях Gen-T Джуниор-ассистент давал более корректный ответ на 20% чаще по сравнению с GPT-3.5. На последних тестах этот показатель увеличился до 55%.\n",
        "\n",
        "Джуниор-ассистент — один из шести высокотехнологичных ИИ-помощников «Вселенной ассистентов» от «Тинькофф». В отличие от традиционных ассистентов, которые обучены решать широкий спектр вопросов, каждый из них обучен быстро решать задачи в своей сфере и обрабатывать сложные запросы"
      ],
      "metadata": {
        "id": "UtOX_8klELpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Предложения по Новым Фичам и Техническая Реализация\n"
      ],
      "metadata": {
        "id": "2fQp3mXqD72s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Фича 1:**\n",
        "\n",
        "Гид по Интересам и Хобби:\n",
        "\n",
        "Описание: Подростки активно ищут новые интересы и хобби. Эта фича помогает им исследовать различные области, находить интересные занятия, получать полезные навыки и знакомиться с единомышленниками. Джуниор выступает в роли гида, который предлагает персонализированные рекомендации, предоставляет информацию о различных хобби и знакомит с сообществами по интересам.\n",
        "\n",
        "Как это может работать:\n",
        "\n",
        "*   База данных хобби и интересов\n",
        "*   Рекомендательная система\n",
        "*   Интеграция с платформами для обучения и предоставление доступа к курсам и урокам\n",
        "*   Персонализированные рекомендации\n",
        "*   Предоставление обзоров, статей и видео о различных хобби и интересах\n",
        "*   Ссылки на сообщества и форумы\n",
        "\n",
        "**Фича 2:**\n",
        "\n",
        "Менеджер Личных Проектов:\n",
        "\n",
        "Описание: Подростки часто занимаются различными проектами – от школьных заданий до хобби и личных инициатив. Эта фича помогает им организовывать свои проекты, ставить цели, планировать шаги, отслеживать прогресс и управлять временем. Джуниор выступает в роли личного менеджера, который напоминает о дедлайнах, предлагает полезные ресурсы и оказывает поддержку.\n",
        "\n",
        "Как это может работать:\n",
        "\n",
        "*   Система управления задачами\n",
        "*   Интеграция с календарем пользователя для отображения дедлайнов и запланированных событий\n",
        "*   Предложение полезных ресурсов на основе тематики проекта\n",
        "*   Визуализация проектов\n",
        "*   Напоминания и уведомления\n",
        "*   Отчеты о прогрессе\n",
        "\n",
        "**Фича 3:**\n",
        "\n",
        "Генератор Гениальных Идей:\n",
        "\n",
        "Описание: Подростки учатся находить инновационные решения для реальных проблем и разрабатывать собственные стартапы. Джуниор выступает в роли генератора идей, предлагая темы для стартапов, проводя мозговые штурмы, помогая разрабатывать бизнес-планы и искать инвесторов.\n",
        "\n",
        "Как это может работать:\n",
        "\n",
        "*   База данных с реальными проблемами, требующими решения\n",
        "*   ИИ-генерация идей стартапов\n",
        "*   Инструменты для поиска инвесторов\n",
        "*   Ссылки на ресурсы для стартапов\n",
        "*   Шаблоны бизнес-планов\n",
        "*   Возможность участвовать в интерактивных мозговых штурмах с ИИ-ассистентом"
      ],
      "metadata": {
        "id": "_JhuE5UNETGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Дообучение Модели для Text2Text Генерации\n"
      ],
      "metadata": {
        "id": "t5x1cHMxCBth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Обоснование выбора модели:**\n",
        "\n",
        "Я выбрал модель mT5 (Multilingual Text-to-Text Transfer Transformer) от Google.\n",
        "\n",
        "*   Она хорошо понимает русский язык и генерирует грамматически правильные и связные тексты.\n",
        "*   mT5 легко доступна в библиотеках Transformers от Hugging Face, что значительно упрощает ее использование и дообучение.\n",
        "*   mT5 показывает хорошие результаты в различных задачах NLP и является хорошим компромиссом между производительностью и вычислительными затратами.\n",
        "\n",
        "**Данные для дообучения:**\n",
        "\n",
        "Данные представляют собой JSONL файл, где каждая строка содержит JSON-объект с диалогом. Каждый диалог состоит из последовательности реплик пользователя и ассистента.\n",
        "\n",
        "**Предобработка данных:**\n",
        "\n",
        "Перед дообучением необходимо:\n",
        "\n",
        "Загрузить JSONL файл и распарсить его в список диалогов.\n",
        "\n",
        "Сформировать пары “вход-выход” для дообучения модели. В качестве входа будет использоваться реплика пользователя, а в качестве выхода - реплика ассистента.\n",
        "\n",
        "Использовать токенизатор mT5 для преобразования текста в числовые представления, понятные модели.\n",
        "\n",
        "Разделить данные на две части: обучающую (для дообучения модели) и валидационную (для оценки качества обучения).\n",
        "\n",
        "**Процесс дообучения:**\n",
        "\n",
        "Загрузка предобученной модели из библиотеки Transformers.\n",
        "\n",
        "Настройка параметров обучения таких как learning rate, batch size, number of epochs, optimizer.\n",
        "\n",
        "Обучение модели\n",
        "\n",
        "Оценка модели на валидационной выборке, чтобы отслеживать прогресс и предотвращать переобучение. Использовать метрики, такие как BLEU, ROUGE, или perplexity.\n",
        "\n",
        "Сохранение модели для дальнейшего использования."
      ],
      "metadata": {
        "id": "xoVFDrxpEaQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate==0.21.0 bitsandbytes==0.40.2 peft==0.5.0 transformers==4.34.0 sentencepiece datasets jsonlines torch\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig, LoraConfig, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "NEW_MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "os.makedirs(\"./offload_dir\", exist_ok=True)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    NEW_MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    offload_folder=\"./offload_dir\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(NEW_MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "original_data = load_dataset(\"IlyaGusev/ru_turbo_saiga\", split=\"train\")\n",
        "\n",
        "additional_data = [\n",
        "    {\n",
        "        \"instruction\": \"Как накопить на новый телефон?\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"Откладывай 10% от карманных денег каждый месяц. В приложении Т-Банка есть копилка — она поможет следить за накоплениями!\",\n",
        "        \"system\": \"Ты — помощник по финансовым вопросам для детей.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Как получить кэшбэк в Т-Банке?\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"Чтобы получить кэшбэк, оплачивай покупки картой Т-Банка. В приложении есть раздел 'Акции' — там можно выбрать магазины с повышенным кэшбэком до 10%!\",\n",
        "        \"system\": \"Ты — дружелюбный ассистент Т-Банка для детей.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "combined_data = concatenate_datasets([original_data, Dataset.from_pandas(pd.DataFrame(additional_data))])\n",
        "\n",
        "def prepare_text(examples):\n",
        "    return {\n",
        "        \"text\": [f\"### Запрос: {q}\\n### Ответ: {a}\" for q, a in zip(examples['instruction'], examples['output'])]\n",
        "    }\n",
        "\n",
        "processed_data = combined_data.map(prepare_text, batched=True)\n",
        "\n",
        "def tokenize_text(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"longest\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "tokenized_data = processed_data.map(tokenize_text, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "lora_settings = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "adapted_model = get_peft_model(base_model, lora_settings)\n",
        "adapted_model.print_trainable_parameters()\n",
        "\n",
        "training_params = TrainingArguments(\n",
        "    output_dir=\"./trained_model\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "model_trainer = Trainer(\n",
        "    model=adapted_model,\n",
        "    args=training_params,\n",
        "    train_dataset=tokenized_data,\n",
        ")\n",
        "\n",
        "model_trainer.train()\n",
        "adapted_model.save_pretrained(\"./trained_model\")\n",
        "tokenizer.save_pretrained(\"./trained_model\")\n",
        "\n",
        "loaded_base = AutoModelForCausalLM.from_pretrained(\n",
        "    NEW_MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "loaded_model = PeftModel.from_pretrained(loaded_base, \"./trained_model\")\n",
        "loaded_model.eval()\n",
        "\n",
        "test_input = tokenizer(\"Как ребенку накопить на новый телефон?\", return_tensors=\"pt\").to(loaded_model.device)\n",
        "test_output = loaded_model.generate(\n",
        "    **test_input,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(test_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bm4zjvySzqil",
        "outputId": "601bfbcd-cdba-459f-99c2-2642a5e4f836"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate==0.21.0 in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: bitsandbytes==0.40.2 in /usr/local/lib/python3.11/dist-packages (0.40.2)\n",
            "Requirement already satisfied: peft==0.5.0 in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: transformers==4.34.0 in /usr/local/lib/python3.11/dist-packages (4.34.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.7)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.5.0) (4.67.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.5.0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (0.17.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (0.14.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines) (25.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'triton.ops'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-7ddf63d9d5eb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenate_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.5.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .auto import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mAutoPeftModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mAutoPeftModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m from .peft_model import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPushToHubMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaskType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# from .config import PeftConfig, PeftType, PromptLearningConfig, TaskType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpeft_types\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaskType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .other import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mTRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mTRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/utils/other.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_hook_to_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_hook_from_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.21.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccelerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m from .big_modeling import (\n\u001b[1;32m      5\u001b[0m     \u001b[0mcpu_offload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcheckpointing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_accelerator_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_custom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_accelerator_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_custom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoaderDispatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_first_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/checkpointing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m     )\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbnb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_4bit_bnb_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_and_quantize_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfsdp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_fsdp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_fsdp_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_fsdp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_fsdp_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m from .launch import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/bnb.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_bnb_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mbitsandbytes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOMPILED_WITH_CUDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mCOMPILED_WITH_CUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# LICENSE file in the root directory of this source tree.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInt8Params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinear8bitLt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStableEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinear4bit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearNF4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearFP4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParams4bit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutlierAwareLinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSwitchBackLinearBnb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtriton_based_modules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSwitchBackLinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSwitchBackLinearGlobal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSwitchBackLinearVectorwise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardLinear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/triton_based_modules.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriton_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_triton_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_rowwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdequantize_rowwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_rowwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantize_rowwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_columnwise_and_transpose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantize_columnwise_and_transpose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/triton/dequantize_rowwise.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_perf_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mearly_config_prune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimate_matmul_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# rowwise quantize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'triton.ops'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}